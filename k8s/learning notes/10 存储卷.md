# 存储卷

​	Volume（存储卷）是Pod中能够被多个容器访问的共享目录。 Kubernetes的Volume概念、用途和目的与Docker的Volume比较类似，但 两者不能等价。首先，Kubernetes中的Volume被定义在Pod上，然后被 一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的 Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终 止或者重启时，Volume中的数据也不会丢失。最后，Kubernetes支持多 种类型Volume，例如GlusterFS、Ceph等先进的分布式文件系统

## 1. 存储卷种类

Kubernetes提供了非常丰富的Volume类型，下面逐一进行说明。 

### **emptyDir** 

一个emptyDir Volume是在Pod分配到Node时创建的。从它的名称就 可以看出，它的初始内容为空，并且无须指定宿主机上对应的目录文 件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也会被永久删除。emptyDir的一些用途如下。 

- 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留。 

-  长时间任务的中间过程CheckPoint的临时保存目录。 

- 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）。

目前，用户无法控制emptyDir使用的介质种类。如果kubelet的配置 是使用硬盘，那么所有emptyDir都将被创建在该硬盘上。Pod在将来可 以设置emptyDir是位于硬盘、固态硬盘上还是基于内存的tmpfs上，上面的例子便采用了emptyDir类的Volume。 

### **hostPath** 

hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以 下几方面。 

- 容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储。 

- 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。 

在使用这种类型的Volume时，需要注意以下几点。 

- 在不同的Node上具有相同配置的Pod，可能会因为宿主机上的目录和文件不同而导致对Volume上目录和文件的访问结果不一致。 

- 如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。 

### **NFS** 

使用NFS网络文件系统提供的共享目录存储数据时，我们需要在系 统中部署一个NFS Server。

### **其他类型的Volume**

- iscsi：使用iSCSI存储设备上的目录挂载到Pod中。 

- flocker：使用Flocker管理存储卷。 

- glusterfs：使用开源GlusterFS网络文件系统的目录挂载到Pod中。

- rbd：使用Ceph块设备共享存储（Rados Block Device）挂载到Pod中。

- gitRepo：通过挂载一个空目录，并从Git库clone一个git repository以供Pod使用。 

- secret：一个Secret Volume用于为Pod提供加密的信息，你可以将定义在Kubernetes中的Secret直接挂载为文件让Pod访问。Secret Volume是通过TMFS（内存文件系统）实现的，这种类型的Volume总是不会被持久化的、
- 云存储：例如awsElasticBlockStore、gcePersistentDisk、AuzreDisk等



## 2. 示例

### 镜像准备

​	这里我们开发一个java的springboot web项目： 具有以下两个功能：

1. 访问/返回项目版本号和服务器域名或主机名
2. 上传、展示图片

#### 1. 编写java服务

```java
package edu.k8s.webapp;

import org.springframework.stereotype.Controller;
import org.springframework.ui.Model;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PathVariable;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.multipart.MultipartFile;

import javax.servlet.ServletOutputStream;
import javax.servlet.http.HttpServletResponse;
import javax.servlet.http.Part;
import java.io.*;
import java.net.InetAddress;
import java.net.UnknownHostException;

@Controller
public class WebController {

    @RequestMapping("/")
    public String index(Model model) throws UnknownHostException {
        InetAddress addr = InetAddress.getLocalHost();
        String hostName= addr.getHostName(); //获取本机计算机名称
        model.addAttribute("hostname",hostName);
        return "index";
    }

    @RequestMapping("/page")
    public String uploadPage(){
        return "upload";
    }

    @RequestMapping("/upload")
    public void upload(@RequestParam(value = "file") Part part) throws IOException {
        InputStream in = part.getInputStream();
        File tempDir = new File("/temp");
        if(!tempDir.exists()){
            boolean mkdir = tempDir.mkdir();
        }
        FileOutputStream out = new FileOutputStream("/temp/" + part.getSubmittedFileName());
        byte[] buff = new byte[1024];
        int len  = 0;
        while((len = in.read(buff))!=-1){
            out.write(buff,0,len);
        }
    }

    @GetMapping("/image/{name}")
    public void show(@PathVariable String name, HttpServletResponse response) throws IOException {
        FileInputStream fileInputStream = new FileInputStream("/temp/" + name);
        ServletOutputStream outputStream = response.getOutputStream();
        byte[] buff = new byte[1024];
        int len = 0;
        while((len = fileInputStream.read(buff))!=-1){
            outputStream.write(buff,0,len);
        }
    }
}
```

服务调试完毕后，执行```mvn clean package```打包

#### 2. 编写Dockerfile

```dockerfile
FROM openjdk:8u201-jdk-alpine3.9
COPY webapp-0.0.1-SNAPSHOT.jar /app.jar
ENTRYPOINT ["java","-jar","/app.jar"]
```

#### 3. 构建上传镜像

这里使用的是个人阿里云本地仓库服务，也可以使用dockerhub等其他镜像仓库服务

```bash
$ docker build -t webapp .
$ docker login --username=赵千里bbzhanshi999 registry.cn-hangzhou.aliyuncs.com
$ docker tag webapp registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
$ docker push registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
```

#### 4. 试运行

在其他节点上下载镜像，运行

```bash
$ docker run --name webapp -p 38080:8080 registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
```

访问服务：http://ipaddr:38080/

![](images/webapptest.PNG)

### 2.1 emptyDir

​	emptyDir是空目录，并且会随着pod的消失而被删除，因此适合存放临时文件，这里我们以webapp来进行示例,创建deploy和NodePort svc

#### 1. 编写 webapp-volumes.yaml

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      volumes:
      - name: tempvol
        emptyDir: {}
      containers:
      - name: webapp
        image: registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - mountPath: /temp
          name: tempvol
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
  namespace: default
spec:
  selector:
    app: webapp
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30088
---
```

#### 2. 运行部署与服务

```bash
$ kubectl apply -f webapp-volumes.yaml
```

访问上传图片的服务，然后手动删除pod，看图片是否还存在

#### 总结

​	我们可以发现，当我们上传图片成功并且访问图片成功后，删除掉pod后，deploy会帮我们重新创建replicas，这时候再去访问图片，你啥也看不到，**说明emptyDir和pod的生命周期绑定**。

### 2.2 hostPath

​	hostpath是将容器的路径挂载在宿主机之上，它虽然能够实现文件的持久化，但是，假如我们重启的pod不原来的node上，那么仍然是访问不到数据的，所以此种方式只适用于绑定宿主机的情况下，例如收集宿主机的日志

#### 1. 编写webapp-hostPath-volumes.yaml

这里将原来的emptyDir修改成hostPath

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      volumes:
      - name: tempvol
        hostPath:
          path: "/opt/webapp/data"
          type: DirectoryOrCreate #目录不存在就自动创建
      containers:
      - name: webapp
        image: registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - mountPath: /temp
          name: tempvol
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
  namespace: default
spec:
  selector:
    app: webapp
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30088
---
```

#### 2. 运行部署与服务

```bash
$ kubectl apply -f webapp-hostPath-volumes.yaml
```

访问上传图片的服务，然后删除节点，访问宿主机下目录 ```/opt/webapp/data```,检查图片是否存在

![](images/hostpath运行效果.PNG)

#### 总结

​	上图中我们可以发现，pod运行在node203节点中，其指定挂载目录下确实有我们上传的图片，当我们删除pod时，这个文件夹依然存在，但是当我们重新创建pod之后，其宿主机如果不在node203上的话，那么图片也就显示不出来。

### 2.3 nfs共享存储

	NFS就是Network File System的缩写，它最大的功能就是可以通过网络，让不同的机器、不同的操作系统可以共享彼此的文件。
	
	NFS服务器可以让PC将网络中的NFS服务器共享的目录挂载到本地端的文件系统中，而在本地端的系统中来看，那个远程主机的目录就好像是自己的一个磁盘分区一样，在使用上相当便利；
​		nfs可以真正的实现存储空间与k8s集群的隔离，数据将被存储中nfs搭建的存储服务中，我们仍然以webapp来做实验

#### 1. 搭建 nfs

1. 单独新建一台宿主机store111作为提供nfs的服务器，并将其域名注入至master和node中（略）

2. store111,node202,node203下载nfs服务

   ```bash
   $ yum install -y nfs-utils
   ```

3. store111 创建共享存储目录

   ```bash
   mkdir -p /data/volumes
   ```

4. 编制/etc/exports文件

   将/data/volumes 暴露至192.168.134.0/16网段，开放读写权限和root权限

   ```bash
   vim /etc/exports
   -------------------
   /data/volumes     192.168.134.0/16(rw,no_root_squash)
   ```

5. 启动服务设置自启动

   ```bash
   $ systemctl start nfs
   $ systemctl enable nfs
   ```

   > nfs默认监听2049端口
   >
   > ```bash
   > ss -tnl
   > --------------------
   > LISTEN      0      64           *:2049      *:*   
   > ```

#### 2. 编辑webapp-nfs-volumes.yaml

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      volumes:
      - name: tempvol
        nfs:
          path: /data/volumes
          server: store111
      containers:
      - name: webapp
        image: registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - mountPath: /temp
          name: tempvol
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
  namespace: default
spec:
  selector:
    app: webapp
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30088
---
```

#### 3. 运行部署与服务

```bash
$ kubectl apply -f webapp-nfs-volumes.yaml
```

访问上传图片的服务，然后删除节点，访问store111下目录 ```/data/volumes```,检查图片是否存在

![](images/nfs运行效果.PNG)

#### 总结

​	我们发现，图片最终上传至store111虚拟机下的/data/volumes目录，nfs给与集群内部的pod有了真正持久化的能力。

## 3. pv与pvc

​	之前提到的Volume是被定义在Pod上的，属于计算资源的一部分， 而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。比如在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中划出一个“网盘”并挂接到虚拟机上。Persistent Volume（PV）和与之相 关联的Persistent Volume Claim（PVC）也起到了类似的作用。PV可以被理解成Kubernetes集群中的某个网络存储对应的一块存储，它与Volume类似，但有以下区别。 

- PV只能是网络存储，不属于任何Node，但可以在每个Node上访问。
- PV并不是被定义在Pod上的，而是独立于Pod之外定义的。 
- PV目前支持的类型包括：gcePersistentDisk、 AWSElasticBlockStore、AzureFile、AzureDisk、FC（Fibre Channel）、Flocker、NFS、iSCSI、RBD（Rados Block Device）、CephFS、 Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMware Photon、Portworx Volumes、ScaleIO Volumes和HostPath（仅供单机测试）。

![](images/pvc存储架构图.PNG)

**pv的好处**：我们可以将存储资源的定义和架设工作从容器编排工作中抽象出来，交由更加专业的fs工程师来进行调试，而开发者只需要声明自己所需要的存储资源就可以了

![](images/pvc人员组织.PNG)

### 搭建与使用过程

1. 搭建底层存储系统
2. 编辑pv与底层存储系统进行绑定
3. 用户定义pod或deploy时创建pvc存储需求声明
4. 运行pod后，pvc会与某个符合条件的pv进行绑定，通过pvc使用底层存储资源

### 示例

​	还是以webapp为例，搭建pv与pvc，底层存储系统仍然采用nfs

#### 1. 创建nfs服务

在store111上创建几个目录

```bash
mkdir -p /data/volumes/v{1,2,3}
```

编辑```/etc/exports```

```bash
/data/volumes/v1     192.168.134.0/16(rw,no_root_squash)
/data/volumes/v2     192.168.134.0/16(rw,no_root_squash)
/data/volumes/v3     192.168.134.0/16(rw,no_root_squash)
```

刷新配置

```bash
exportsfs -arv

#检查挂载
showmount -e
----------------
/data/volumes/v3 192.168.134.0/16
/data/volumes/v2 192.168.134.0/16
/data/volumes/v1 192.168.134.0/16
```

#### 2. 定义pv

编制资源清单： pv-demo.yaml

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001
  labels:
    name: pv001
spec:
  nfs:
    path: /data/volumes/v1
    server: store111
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 100Mi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv002
  labels:
    name: pv002
spec:
  nfs:
    path: /data/volumes/v2
    server: store111
  accessModes: ["ReadWriteOnce"]
  capacity:
    storage: 200Mi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv003
  labels:
    name: pv003
spec:
  nfs:
    path: /data/volumes/v3
    server: store111
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 200Mi
```

创建pv

```bash
$ kubectl apply -f pv-demo.yaml
```

#### 3. 定义 pvc和声明使用pvc

编辑 webapp-pvc-volumes.yaml

```yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: imagepvc
  namespace: default
spec:
  accessModes: ["ReadWriteMany"]
  resources:
    requests:
      storage: 150Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deploy
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      volumes:
      - name: tempvol
        persistentVolumeClaim:
          claimName: imagepvc
      containers:
      - name: webapp
        image: registry.cn-hangzhou.aliyuncs.com/zhaoqianli/webapp:v1
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - mountPath: /temp
          name: tempvol
        imagePullPolicy: IfNotPresent
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-svc
  namespace: default
spec:
  selector:
    app: webapp
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    nodePort: 30088
---
```

#### 4 运行pvc和pv

```bash
$ kubectl apply -f webapp-pvc-volumes.yaml
```

#### 5. 查看pv状态

```bash
$ kubectl get pv
------------------
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS   REASON   AGE
pv001   100Mi      RWO,RWX        Retain           Available                                              3m18s
pv002   200Mi      RWO            Retain           Available                                              3m18s
pv003   200Mi      RWO,RWX        Retain           Bound       default/imagepvc                           3m18s
```

我们发现pvc最终和pv003挂钩了，原因是因为其storage符合要求，并且包含了多谢多次的模式

#### 6  测试服务

访问服务，查看store111下v3目录中是否有图片

![](images/pvc运行效果.PNG)

可见此时对应的v3文件夹中已经包含了上传的图片

